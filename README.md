# LLM Arbitration Research - Vis Moot Submissions

This repository contains the complete research materials from a study comparing AI-generated legal submissions with winning human teams from the 2024 Vis Moot competition.

## Research Overview

This study investigates whether frontier LLMs can draft Vis Moot submissions as persuasively and accurately as 2024's winning universities from 373 teams. The research tests three key assumptions about LLMs in legal practice:

1. **Productivity gains** - Can AI produce legal work faster?
2. **Accuracy concerns** - How reliable are AI-generated legal citations?
3. **Legal reasoning capability** - Can AI perform genuine legal analysis?

### Key Findings

**üèÜ Performance Results:**
- **Claude 3.7 Sonnet's** claimant memorial ranked alongside the 1st and 2nd human teams, ahead of 3rd and 4th place winners
- Other LLMs ranked below human submissions
- Demonstrated emerging AI persuasion capabilities rivaling top law students

**üìä Accuracy Results:**
- **Poor citation accuracy**: Only 9%-36% of legal citations were pinpoint accurate
- Significant reliability issues with legal precedents and references

**‚ö° Productivity Results:**
- **Dramatic time savings**: LLMs produced complete submissions in under 9.5 hours with one person
- **Human comparison**: Traditional teams averaged 6.2 people working for months
- Clear productivity advantages for legal research and drafting

### The Case Background

All submissions address the same fictional international commercial arbitration case involving:
- **SensorX plc** (technology company selling sensors)
- **Visionic Ltd** (automotive company buying sensors)  
- A cyber fraud incident that misdirected payment
- Questions of contractual obligations and liability
- International sales law and arbitration procedures

### Technical Notes

- All submissions were generated using customized prompts
- Models had access to the same case materials
- Outputs were anonymized before evaluation
- Blind ranking was conducted by 2 legal students competing in the 2025 Vis Moot
- No model had access to actual case law databases during generation


### Research Significance

This study represents a significant milestone in AI legal capabilities research, demonstrating that:
- **AI can produce sophisticated legal arguments** comparable to top law students
- **Citation accuracy remains a critical weakness** requiring human oversight
- **Productivity gains are substantial** but quality control is essential
- **Legal education may need adaptation** to address AI capabilities
- **The legal profession faces transformation** in research and drafting practices

## Repository Structure

### üìÅ A - Prompt Engineering
Contains the carefully crafted prompts used to generate the legal submissions:

- **`claimant_final_prompt.md`** - Final optimized prompt for claimant position arguments
- **`claimant_initial_prompt.md`** - Early version of claimant prompt showing iteration process
- **`respondent_final_prompt.md`** - Final optimized prompt for respondent position arguments

*These prompts were refined through testing to produce the highest quality legal argumentation.*

### üìÅ B - Final LLM Outputs ‚≠ê
The main research outputs - complete legal submissions generated by each AI model:

**Claimant Submissions (Party Bringing the Case):**
- **`Claimant_Claude-3_7.md/.docx`** - Top performer, ranked with 1st/2nd place human teams
- **`Claimant_GPT4o.md/.docx`** - OpenAI's GPT-4o submission
- **`Claimant_Gemini-2-pro-experiment.md/.docx`** - Google's Gemini 2.0 Pro submission

**Respondent Submissions (Defending Party):**
- **`Respondent_GPT4o.md/.docx`** - OpenAI's defense arguments
- **`Respondent_Gemini-2-pro-experiment.md/.docx`** - Google's defense arguments

### üìÅ C - Initial LLM Outputs
Early experimental outputs showing the development process:

- **`initial_claimant_output_gemini-2-pro.md`** - Early Gemini claimant attempt
- **`initial_respondent_output_GPT4o-search.md`** - GPT-4o respondent with search capabilities
- **`initial_respondent_output_o3-mini-high.md`** - OpenAI o3-mini high reasoning mode attempt

*These show the iterative process of refining prompts and approaches, in particular why search models and reasoning models were excluded.*

### üìÅ D - Email Correspondence
Documentation of the research methodology and evaluation process:

- **`email_to_legal_ai.md`** - Outreach to legal AI companies for tool evaluation
- **`email_to_usyd_2025_vis_team.md`** - Request to law students for blind evaluation of submissions

### üìÅ E - Results
Quantitative analysis and evaluation data:

- **`accuracy_compiled_results.xlsx`** - Comprehensive citation accuracy analysis
- **`accuracy_results_pretty.html`** - Visual presentation of accuracy findings
- **`evaluator_rankings.xlsx`** - Blind rankings by law student evaluators

### üìÅ F - Miscellaneous
Supporting research materials and technical documentation:

- **`AI_legal_tool_outcomes`** - Survey of 16 legal AI tools and their research suitability
- **`api_coding.md`** - Technical challenges encountered with API implementation
- **`extract_text_citations_into_csv.py`** - Python script for citation accuracy analysis
- **`public_statements_by_model_providers_on_training_dataset.md`** - AI training data transparency research

