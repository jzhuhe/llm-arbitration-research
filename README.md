# LLM Arbitration Research - Vis Moot Submissions

This repository contains the outputs of Large Language Models (LLMs) that participated in a research study comparing AI-generated legal submissions with winning human teams from the 2024 Vis Moot competition.

## What's in This Repository

This repository contains the complete legal submissions generated by each AI model for both claimant and respondent positions. Each submission represents a complete Vis Moot memorial with full legal argumentation, citations, and procedural analysis.

### File Guide

Each AI model generated two submissions - one arguing for the **Claimant** (the party bringing the case) and one for the **Respondent** (the party defending against the case). This mirrors how real legal teams prepare arguments for both sides.

**üìÅ File Naming Convention:**
- `Claimant_[Model-Name]` - Arguments supporting the party bringing the case
- `Respondent_[Model-Name]` - Arguments defending against the case
- `.md` files - Text versions (recommended for reading)
- `.docx` files - Microsoft Word versions (for formal presentation)

**ü§ñ Available Models:**
- **Claude-3_7** - Anthropic's Claude 3.7 Sonnet (top performer)
- **GPT4o** - OpenAI's GPT-4o
- **Gemini-2-pro-experiment** - Google's Gemini 2.0 Pro Experimental

### Technical Notes

- All submissions were generated using customized prompts
- Models had access to the same case materials
- Outputs were anonymized before evaluation
- Blind ranking was conducted by 2 legal students who were in competing in the 2025 Vis Moot
- No model had access to actual case law databases during generation


## About the Research

This study investigates whether frontier LLMs can draft Vis Moot submissions as persuasively and accurately as 2024's winning universities from 373 teams. The research tests three key legal assumptions about LLMs:

1. **Productivity gains** - Can AI produce legal work faster?
2. **Accuracy concerns** - How reliable are AI-generated legal citations?
3. **Legal reasoning capability** - Can AI perform genuine legal analysis?

### Research Methodology

The study used three leading AI models with customized prompts:
- **GPT-4o** (OpenAI)
- **Gemini 2.0 Pro Experimental** (Google)
- **Claude 3.7 Sonnet** (Anthropic)

All outputs were anonymized and blind-ranked against winning human submissions from the 2024 Vis Moot competition.

### Key Findings

**üèÜ Performance Results:**
- **Claude 3.7 Sonnet's** claimant memorial ranked alongside the 1st and 2nd human teams, ahead of 3rd and 4th place winners
- Other LLMs ranked below human submissions
- Demonstrated emerging AI persuasion capabilities rivaling top law students

**üìä Accuracy Results:**
- **Poor citation accuracy**: Only 9%-36% of legal citations were pinpoint accurate
- Significant reliability issues with legal precedents and references

**‚ö° Productivity Results:**
- **Dramatic time savings**: LLMs produced complete submissions in under 9.5 hours with one person
- **Human comparison**: Traditional teams averaged 6.2 people working for months
- Clear productivity advantages for legal research and drafting

**üß† Legal Reasoning:**
- Evidence that advanced LLMs may demonstrate ability to mimic sophisticated legal reasoning
- Questions remain about genuine understanding vs. pattern matching

