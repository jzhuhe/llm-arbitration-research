OpenAI’s team indicated they used the following databases when training GPT-3:  Common Crawl (filtered), WebText2, Books1, Books2, and Wikipedia. Common Crawl contains a large amount of webpage data,  OpenWebText2 contains all Reddit submissions from 2005 up until April 2020,  Wikipedia is a publicly available, online encyclopedia.  It is possible Books1 and Books2 contain arbitration related material. OpenAI only states that Books1 and Books2 are ‘internet-based books corpora’  and information about them is scarce. 

Google’s team that developed Gemini stated “our pre-training dataset uses data from web documents, books, and code, and includes image, audio, and video data.” 

Meta’s team that developed LLama disclosed the following composition of training data: 
●	CommonCrawl 67.0% 3.3 TB
●	C4 15.0% 783 GB
●	Github 4.5% 328 GB
●	Wikipedia 4.5% 83 GB
●	Books 4.5% 85 GB
●	ArXiv 2.5% 92 GB
●	StackExchange 2.0% 78 GB